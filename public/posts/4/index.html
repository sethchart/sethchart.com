<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Latent Dirichlet Allocation | Seth Chart</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="LDA , NLP , Machine Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Latent Dirichlet Allocation" />
  <meta name="twitter:description" content=""
  />
  <meta name="twitter:site" content="@https://twitter.com/sethchart" />
  <meta name="twitter:creator" content="@https://twitter.com/sethchart" />
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />

  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.679ea46872c2b319c5ae942c601a7330924fd57451c89d621d6cb129c870c2a8.css" integrity="sha256-Z56kaHLCsxnFrpQsYBpzMJJP1XRRyJ1iHWyxKchwwqg="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.d4f5ff97123043c4601a563af6d280aa37573addb6592364613a791e1cc035d2.css" integrity="sha256-1PX/lxIwQ8RgGlY69tKAqjdXOt22WSNkYTp5HhzANdI="/>
  
  
   
   
    

<script type="application/ld+json">
  
    { 
      "@context": "http://schema.org", 
      "@type": "WebSite", 
      "url": "https:\/\/sethchart.com\/posts\/4\/",
      "name": "Latent Dirichlet Allocation",
      "author": {
        "@type": "Person",
        "name": ""
      },
      "description": ""
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" role="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/posts/0">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/posts">blog</a>
      </li>
    
      <li>
        <a  href="/">home</a>
      </li>
    
      <li>
        <a  href="/resume-sethchart.pdf">resume</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Latent Dirichlet Allocation</h1>
            <time datetime="2021-02-14 13:24:44 -0500 EST" class="post__date"
            >2021-02-14</time>
          </header>
          <article class="post__content">
              
<h2 id="an-example">An Example<a class="anchor" href="#an-example">#</a></h2>
<p>In one of my recent projects I wanted to investigate job postings for jobs in the data industry.
In the data industry, it is widely believed that job titles are not a reliable indicator of the role that they describe.
The title Data Scientist may describe two completely different roles depending on the company.
Similarly, nearly identical roles might have the job title Data Scientist in one company and Data Analyst in another company.
I wanted to investigate this belief by using unsupervised learning to &lsquo;read&rsquo; full job descriptions, cluster them into similar roles, and then investigate the relationship between these roles and the stated job titles.
If you would like to learn more about that project please check our the project repo <a href="https://github.com/sethchart/DataJobs">here</a>.</p>
<p>In this post, I want to focus in on the process of &lsquo;reading&rsquo; the job descriptions.
Ideally, an algorithm that reads the job descriptions will take the full text of the job description and return a simple and meaningful summary that can be used to group similar job descriptions together.</p>
<h2 id="how-would-a-human-read">How Would a Human Read?<a class="anchor" href="#how-would-a-human-read">#</a></h2>
<p>If was forced to read a large collection of job description and group similar roles together by hand, I would probably read through the job description and try to find all of the distinct skills required by the role and summarize them as a short list. Then, I would group jobs together that required similar skills.
Most likely, each job description would describe several skills with varying emphasis.</p>
<h2 id="what-is-latent-dirichlet-allocation">What is Latent Dirichlet Allocation?<a class="anchor" href="#what-is-latent-dirichlet-allocation">#</a></h2>
<p>Latent Dirichlet Allocation (LDA), is a model that approximates the reading process above quite well.
Essentially, we give LDA a collection of documents and ask for it to detect common topics within the documents.
We must supply LDA with the number of topics to detect.
Once LDA has been trained on the collection of documents, it summarizes each document by describing the mixture of topics that it describes.
Even better, a trained model can summarize a new document that it has never seen.
Note that, a new document should be similar to the training data.
An LDA model trained on job descriptions has no business reading takeout menus.</p>
<p>So how does this apply to the task of summarizing required skills by reading job descriptions?
Well the topics in well written job descriptions should be, for the most part, required skills. Therefore, LDA should detect required skills when it is trained on job descriptions.</p>
<h2 id="how-does-lda-work">How Does LDA Work?<a class="anchor" href="#how-does-lda-work">#</a></h2>
<p>In this section I want to give a fairly non-technical overview of LDA ignoring most of the probabilistic details.</p>
<h3 id="how-does-lda-see-a-document">How does LDA See a Document?<a class="anchor" href="#how-does-lda-see-a-document">#</a></h3>
<p>The first thing that we should know about LDA is that it does not know anything about word order.
The input for an LDA model is a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words</a>, which reduces a document to a word frequency distribution.
For example, consider the document below, consisting of two sentences.</p>
<pre><code class="language-python">doc1 = &quot;Seth likes to go for walks. Salimah likes walks too.&quot;
</code></pre>
<p>When we convert the document to a bag-of-words, we get the following.</p>
<pre><code class="language-python">bow1 = { 
    &quot;seth&quot;: 1, &quot;likes&quot;: 2, &quot;to&quot;: 1, &quot;go&quot;: 1, 
    &quot;for&quot;: 1, &quot;walks&quot;: 2, &quot;salimah&quot;: 1, &quot;too&quot;: 1 
}

</code></pre>
<h3 id="what-about-a-collection-of-documents">What About a Collection of Documents?<a class="anchor" href="#what-about-a-collection-of-documents">#</a></h3>
<p>Generally we apply LDA to a collection of documents, often called a <em>corpus</em>.
Let&rsquo;s add two more documents to our corpus.</p>
<pre><code class="language-python">doc2 = &quot;Salimah likes to travel.&quot;

doc3 = &quot;&quot;&quot;Salimah likes pasta more than ramen.
Seth likes ramen more than pasta.&quot;&quot;&quot;
</code></pre>
<p>The corresponding bag-of-words representations are below.</p>
<pre><code class="language-python">bow2 = { 
    &quot;salimah&quot;: 1, &quot;likes&quot;: 1, &quot;to&quot;: 1, &quot;travel&quot;: 1
}

bow3 = { 
    &quot;salimah&quot;: 1, &quot;likes&quot;: 2, &quot;pasta&quot;: 2, 
    &quot;more&quot;: 2, &quot;than&quot;: 2, &quot;ramen&quot;: 2, &quot;seth&quot;: 1
}
</code></pre>
<p>Now or corpus is the set of all three documents:</p>
<pre><code class="language-python">corpus = { doc1, doc2, doc3 }
</code></pre>
<p>The set of all words that appear in any document in our corpus is called our <em>vocabulary</em>.
For our example corpus the vocabulary is the set of words below.</p>
<pre><code class="language-python">vocab = {
   &quot;seth&quot;, &quot;likes&quot;, &quot;to&quot;, &quot;go&quot;, &quot;for&quot;, &quot;walks&quot;,
   &quot;salimah&quot;, &quot;too&quot;, &quot;travel&quot;, &quot;pasta&quot;, &quot;more&quot;,
   &quot;than&quot;, &quot;ramen&quot;
}
</code></pre>
<h3 id="a-model-for-writing">A Model for Writing<a class="anchor" href="#a-model-for-writing">#</a></h3>
<p>Let&rsquo;s imagine that when I wrote the documents above I had one of three topics in mind: exercise, food, or hobbies.
When writing the first document I had the topic of exercise in mind, so I selected words that were relevant to that topic to include.
Similarly, the second document is on the topic of food and the third is on the topic of hobbies.
In this case, each document is essentially focused on a single topic.</p>
<p>Suppose that I tell you that I am about to write a new document on the topic of food, using the vocabulary from our corpus.
It seems probable that I will use the words <code>pasta</code> and <code>ramen</code>, but much less probable that I will use the word <code>walk</code>.
If instead, I told you that the topic of the new document would be exercise, then it seems probable that I will use the word <code>walk</code>, but less probable that I will use the word <code>ramen</code>.</p>
<p>A document does not need to have a single topic.
To use our examples efficiently, let&rsquo;s imagine that instead of three topics, I had two topics in mind: Seth&rsquo;s preferences and Salimah&rsquo;s preferences.
Both Document 1 and Document 3 describe a mixture of my two topics, while Document 2 only describes Salimah&rsquo;s preferences.</p>
<p>An LDA model makes the following assumptions about a corpus of documents.</p>
<ol>
<li>There is a fixed vocabulary of words that were used to construct the documents in our corpus.</li>
<li>There is a fixed set of topics that are described by the documents in our corpus.</li>
<li>For each word in our vocabulary, we can assign a conditional probability that the word will be included in a document given the current topic.</li>
<li>For each topic, we can assign a probability that it will be described by a document in our corpus.</li>
</ol>
<p>Based on these assumptions, an LDA model assumes that each document is written according to the following process.</p>
<ol>
<li>Select the length of the document at random.</li>
<li>Select a distribution of topics for the document according to the corpus wide distribution.</li>
<li>For each word in the document, first select a particular topic according to the distribution from Step 2, then select a word from the vocabulary according to the conditional distribution on the vocabulary given the current topic.</li>
</ol>
<h2 id="fitting-an-lda-model">Fitting an LDA Model<a class="anchor" href="#fitting-an-lda-model">#</a></h2>
<p>Above we have described at length the underling assumptions of an LDA model, particularly the model for writing a document described in the last section.
The issue that we still need to resolve has to do with what we know when we are handed a corpus of documents.</p>
<p>For each document in our corpus, we can observe the frequency of each word from our vocabulary.
However, we do not know the distribution of topics for our corpus, the conditional distributions on the vocabulary given the topic, or the distribution of topics described in each document.</p>
<p>We are primarily interested in the distribution of topics that are described in a document.
However, to access this information we also need to determine the topic distribution and the conditional word distributions.</p>
<p>There are two main computational tasks that must be completed to obtain topic distributions from documents.</p>
<ol>
<li>Estimate the distribution of topics and conditional distributions of words that maximize the log likelihood of the training data.</li>
<li>Given the distributions from Step 1, estimate the distribution of topics described by a particular document.</li>
</ol>
<p>Provided that we have completed Step 1 and a document is reasonably similar to the training data, we can execute Step 2 on a document that was not part of the training data.</p>
<p>A deeper explanation of the steps above requires a substantially more detailed description of the probabalistic tools that are used and is beyond the scope of this post.
We refer the interested reader to the reference below, which was the inspiration for this post.</p>
<h2 id="conclusion">Conclusion<a class="anchor" href="#conclusion">#</a></h2>
<p>LDA models provide a powerful unsupervised tool for detecting topics within a corpus of documents.
They support documents that describe a mixture of topics.
The number of topics must be selected by the prectitioner and tuning the number of topics can be a computationaly intensive process.
Because an LDA model is based on a realistic model of document creation and explicitly incorporates the concept of topics, this type of model can provide more intuitive results than methods that rely on word embedding geoometry.</p>
<h2 id="references">References<a class="anchor" href="#references">#</a></h2>
<p><a href="https://dl.acm.org/doi/pdf/10.5555/944919.944937">David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3, null (3/1/2003), 993–1022.</a></p>


              
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="https://sethchart.com/posts/3/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">How I Scraped Job Postings from Careerjet</span>
    </a>
  

  
    <a class="pagination__item" href="https://sethchart.com/posts/we-write-a-custom-tokenizer-for-scikit-learns-countvectorizer-that-implements-lemmatization-with-parts-of-speech-tagging./">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Adding Lemmatization to CountVectorizer Without Making a Mess</a>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a class="social-icons__link" title="Twitter"
         href="https://twitter.com/sethchart"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://sethchart.com/svg/twitter.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" title="GitHub"
         href="https://github.com/sethchart"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://sethchart.com/svg/github.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" title="Email"
         href="mailto:seth.chart@protonmail.com"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://sethchart.com/svg/email.svg')"></div>
      </a>
    
  
     
    
  
     
    
  
     
    
  
     
    
      <a class="social-icons__link" title="LinkedIn"
         href="https://www.linkedin.com/in/sethchart"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://sethchart.com/svg/linkedin.svg')"></div>
      </a>
    
  
     
    
     
</div>

            <p>Seth Chart© 2021</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.49e4d8a384357d9b445b87371863419937ede9fa77737522ffb633073aebfa44.js" integrity="sha256-SeTYo4Q1fZtEW4c3GGNBmTft6fp3c3Ui/7YzBzrr&#43;kQ=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
